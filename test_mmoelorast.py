#!/usr/bin/env python3
"""
æµ‹è¯• MMOELoraST æ¨¡å—çš„å¯¼å…¥å’ŒåŸºæœ¬åŠŸèƒ½
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_mmoelorast_import():
    """æµ‹è¯• MMOELoraST æ¨¡å—çš„å¯¼å…¥"""
    try:
        from Model.MLoRA.peft.tuners.mmoelorast import (
            MMOELoraSTLinear, 
            MMOELoraSTConfig,
            Expert,
            Gate,
            MMOELinearA,
            MMOELinearB
        )
        print("âœ“ MMOELoraST æ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— MMOELoraST æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_expert_creation():
    """æµ‹è¯• Expert ç±»çš„åˆ›å»º"""
    try:
        from Model.MLoRA.peft.tuners.mmoelorast import Expert
        
        expert = Expert(in_features=64, out_features=32)
        x = torch.randn(2, 64)
        output = expert(x)
        
        assert output.shape == (2, 32), f"Expected shape (2, 32), got {output.shape}"
        print("âœ“ Expert ç±»åˆ›å»ºå’Œæ¨ç†æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— Expert ç±»æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gate_creation():
    """æµ‹è¯• Gate ç±»çš„åˆ›å»º"""
    try:
        from Model.MLoRA.peft.tuners.mmoelorast import Gate
        
        # åˆ›å»ºä¸´æ—¶çš„ pattern æ–‡ä»¶
        pattern_embed = torch.randn(8, 128)
        torch.save(pattern_embed, 'temp_pattern.pt')
        
        gate = Gate(
            input_size=64, 
            expert_num=8, 
            gate_embed_dim=128, 
            gate_embed_path='temp_pattern.pt',
            expert_top_k=2
        )
        
        x = torch.randn(2, 10, 64)
        output = gate(x)
        
        assert output.shape == (2, 8), f"Expected shape (2, 8), got {output.shape}"
        print("âœ“ Gate ç±»åˆ›å»ºå’Œæ¨ç†æˆåŠŸ")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove('temp_pattern.pt')
        return True
    except Exception as e:
        print(f"âœ— Gate ç±»æµ‹è¯•å¤±è´¥: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists('temp_pattern.pt'):
            os.remove('temp_pattern.pt')
        return False

def test_mmoelinear_creation():
    """æµ‹è¯• MMOELinearA å’Œ MMOELinearB ç±»çš„åˆ›å»º"""
    try:
        from Model.MLoRA.peft.tuners.mmoelorast import MMOELinearA, MMOELinearB
        
        # æµ‹è¯• MMOELinearA
        mmoe_a = MMOELinearA(in_features=64, out_features=32, expert_num=4)
        x = torch.randn(2, 64)
        outputs_a = mmoe_a(x)
        
        assert len(outputs_a) == 4, f"Expected 4 outputs, got {len(outputs_a)}"
        assert all(out.shape == (2, 8) for out in outputs_a), "Output shapes mismatch"
        
        # æµ‹è¯• MMOELinearB
        mmoe_b = MMOELinearB(in_features=32, out_features=16, expert_num=4)
        outputs_b = mmoe_b(outputs_a)
        
        assert len(outputs_b) == 4, f"Expected 4 outputs, got {len(outputs_b)}"
        assert all(out.shape == (2, 16) for out in outputs_b), "Output shapes mismatch"
        
        print("âœ“ MMOELinearA å’Œ MMOELinearB ç±»åˆ›å»ºå’Œæ¨ç†æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âœ— MMOELinear ç±»æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯• MMOELoraST æ¨¡å—...")
    
    tests = [
        test_mmoelorast_import,
        test_expert_creation,
        test_gate_creation,
        test_mmoelinear_creation
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
        print()
    
    print(f"æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼MMOELoraST æ¨¡å—å·¥ä½œæ­£å¸¸ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³ä»£ç ã€‚")

if __name__ == "__main__":
    main() 